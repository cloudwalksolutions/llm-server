# llama-server Configuration
# Copy this file to config/llama.yaml and customize for your setup

# Remote server connection (password set in .env file)
remote:
  host: 192.168.1.168
  user: walkerobrien

# llama-server performance parameters
llama:
  # Concurrency: number of parallel request slots
  # Higher = more concurrent requests, more memory usage
  # Recommended: 6-12 for 16-core CPU
  parallel_slots: 8

  # Batching: controls batch processing sizes
  # batch_size: logical batch (pipeline parallelization)
  # ubatch_size: physical batch (GPU batch, 4:1 ratio recommended)
  batch_size: 2048
  ubatch_size: 512

  # Threading: allocate CPU threads for different tasks
  # Total threads available: 32 (16 cores Ã— 2 threads)
  # Leave ~25% for system overhead
  threads_gen: 12      # Generation (primary inference)
  threads_batch: 8     # Batch/prompt processing
  threads_http: 6      # HTTP request handling

  # Caching: prompt cache reuse threshold
  # Lower = more aggressive caching (better for similar prompts)
  # Higher = less caching (better for diverse prompts)
  cache_reuse: 256

  # Context window size in tokens
  # Options: 8192, 16384, 32768
  # Higher = more memory usage, longer prompts supported
  context_size: 32768

# System resource limits (usually don't need to change)
resources:
  limit_nofile: 65535
  limit_memlock: infinity

# Benchmarking configuration
benchmark:
  concurrent_requests: 8    # Match parallel_slots for testing
  max_tokens: 100
  timeout: 120
